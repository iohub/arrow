\section{Implementation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:impl}

The traditional object-oriented approach to implementing 
first-class patterns is based on run-time compositions through 
interfaces. This ``\emph{patterns as objects}'' approach has been 
explored in several different languages~\cite{Visser06matchingobjects,geller2010pattern,FuncCSharp,Grace2012}.
Implementations differ in where bindings are stored and what is returned as a 
result, but in its most basic form it consists of the 
\code{pattern} interface with a virtual function \code{match} that accepts a subject 
and returns whether it was accepted or rejected.
This approach is open to new patterns and pattern combinators, but a mismatch in the type of the subject and the 
type accepted by the pattern can only be detected at run-time.
Furthermore, it implies significant run-time overhead (\textsection\ref{sec:patcmp}).

%% While the approach is open to new patterns and pattern combinators (the patterns 
%% are composed at run-time by holding references to other pattern objects), it has 
%% some design problems. For example, mismatch in the type of the subject and the 
%% type accepted by the pattern can only be detected at run-time, while in 
%% languages with built-in support of pattern matching it is typically detected at 
%% type-checking phase. The approach may also unnecessarily clutter the code by 
%% requiring lots of similar boilerplate code be written. For example, modeling n+k 
%% patterns requires additional interface for evaluating the \code{expression}. 
%% With it, we have a dilemma of whether \code{expression} should be derived from 
%% \code{pattern}, \code{pattern} from \code{expression}, or neither of those. 
%% Independently of the choice, implementation of pattern combinators will require 
%% that the class of the combinator conditionally derives from \code{pattern}, 
%% \code{expression} or both depending on which of these interfaces its arguments 
%% implement. On one hand, this will require a separate implementation of the 
%% combinator for each of the cases, while on the other it makes the combinators 
%% dependent on something that was only needed to implement n+k patterns.

%To quantify the overhead somewhat, we reimplemented the factorial function from 
%\textsection\ref{sec:cpppat} using object patterns and timed a million 
%computations of factorial on arguments ranging from 0 to 10. Depending on the 
%argument, the approach based on object patterns was 12-22 times slower than 
%factorial based on \emph{Mach7}. Note that for this experiment we took extra care to 
%not allocate patterns or intermediary objects on the heap, made sure the bodies 
%of all virtual functions were also available for inlining since we composed 
%objects on the stack and thus their complete types were known. We also used a 
%faster \code{typeid}-check instead of a slower \code{dynamic_cast} to ensure the 
%safety of unpacking an object. Finally, we repeated the experiment while removing 
%the safety check altogether (assuming the argument will be of the correct 
%dynamic type) and could reduce the overhead to 6.74-18 times, which is still too 
%costly to be considered a viable solution for a modern \Cpp{} use. We show in 
%\textsection\ref{sec:patcmp} that \emph{Mach7} patterns produce code that is only few 
%percentage points slower than manualy handcrafted code without patterns. 

\subsection{Patterns as Expression Templates}
\label{sec:pat}

Patterns in \emph{Mach7} are also represented as objects; however, they are 
composed at compile time, based on \Cpp{} concepts. 
\term{Concept} is the \Cpp{} community's long-established term for a set of 
requirements for template parameters. Concepts were not included in \Cpp{}11, 
but~techniques for emulating them with 
\code{enable\_if}~\cite{jarvi:03:cuj_arbitrary_overloading} have been in use for 
a while. \code{enable\_if} provides the ability to \emph{include} or 
\emph{exclude} certain class or function declarations from the compiler's 
consideration based on conditions defined by arbitrary metafunctions.
To~avoid the verbosity of \code{enable\_if}, in this work we use the notation 
for \term{template constraints} -- a simpler version of concepts~\cite{N3580}.
The \emph{Mach7} implementation emulates these constraints.

There are two main constraints on which the entire library is built: 
\code{Pattern} and \code{LazyExpression}.

\begin{lstlisting}[keepspaces]
template <typename P> constexpr bool Pattern() {
  return Copyable<P>         // P must also be Copyable
      && is_pattern<P>::value // this is a semantic constraint
      && requires (typename S, P p, S s) {// syntactic reqs:
           bool = { p(s) };     // usable as a predicate on S
           AcceptedType<P,S>; // has this type function
};       }
\end{lstlisting}

%It requires that any type \code{P} modeling \code{Pattern} concept must also 
%model \code{Copyable} concept, be explicitly marked as pattern via 
%\code{is_pattern} trait as well as be

\noindent
The \code{Pattern} constraint is the analog of the \code{pattern} interface from the 
\emph{patterns as objects} solution. Objects of any class \code{P} satisfying 
this constraint are patterns and can be composed with any other patterns in the 
library as well as be used in the \code{Match} statement. 

Patterns can be passed as arguments of a function, so they must be
\code{Copyable}. Implementation of pattern combinators requires the 
library to overload certain operators on all the types satisfying the \code{Pattern}
constraint. To avoid overloading these operators for types that satisfy the 
requirements accidentally, the \code{Pattern} constraint is a \emph{semantic constraint}, 
which means that classes claiming to satisfy it have to state that explicitly by specializing the
\code{is_pattern<P>} trait. The constraint also introduces some \emph{syntactic 
requirements}, described by the \code{requires} clause. In particular, because 
patterns are predicates on their subject type, they require presence of an 
application operator that checks whether a pattern matches a given subject. 
Unlike the \emph{patterns as objects} approach, the \code{Pattern} constraint does not impose 
any restrictions on the subject type \code{S}. Patterns like the wildcard 
pattern will leave the \code{S} type completely unrestricted, while other 
patterns may require it to satisfy certain constraints, model a given concept, 
inherit from a certain type, etc. 
The application operator will typically return a value of type \code{bool} 
indicating whether the pattern is \subterm{pattern}{accepted} on a given subject 
or \subterm{pattern}{rejected}. %For convenience reasons, 
%application operator is allowed to return any type that is convertible to 
%\code{bool} instead, e.g. a pointer to a casted subject, which is useful in 
%emulating the support of \subterm{pattern}{as-patterns}.

Most of the patterns are applicable only to subjects of a given \subterm{type}{expected type} 
or types convertible to it. This is the case, for example, with value and  
variable patterns, where the expected type is the type of the underlying value, 
as well as with the constructor pattern, where the expected type is the type 
being decomposed. Some patterns, however, do not have a single 
expected type and may work with subjects of many unrelated types. A wildcard 
pattern, for example, can accept values of any type without involving a 
conversion. To account for this, the \code{Pattern} constraint requires the presence of 
a type alias \code{AcceptedType}, which given a pattern of type \code{P} and 
a subject of type \code{S} returns an expected type \code{AcceptedType<P,S>} 
that will accept subjects of type \code{S} with no or a minimum of conversions. 
By default, the alias is defined in terms of a nested type function 
\code{accepted_type_for}, as follows:

\begin{lstlisting}
template<typename P, typename S>
  using AcceptedType = P::accepted_type_for<S>::type;
\end{lstlisting}

\noindent
The wildcard pattern defines \code{accepted_type_for} to be an identity 
function, while variable and value patterns define it to be their underlying 
type. The constructor pattern's accepted type is the type it decomposes, which 
is typically different from the subject type. \emph{Mach7} employs an efficient 
type switch~\cite{TS12} under the hood to convert subject type to accepted type. 

Guards, n+k patterns, the equivalence combinator, and potentially some 
new user-defined patterns depend on capturing the structure (term) of lazily-evaluated expressions.
All such expressions are objects of some type \code{E} 
that must satisfy the \code{LazyExpression} constraint:

\begin{lstlisting}[keepspaces]
template <typename E> constexpr bool LazyExpression() {
  return Copyable<E>             // E must also be Copyable
      && is_expression<E>::value // this is semantic constraint
      && requires (E e) {        // syntactic requirements:
           ResultType<E>;          // associated result_type
           ResultType<E> == { eval(e) };// eval(E)->result_type
           ResultType<E> { e };  // conversion to result_type
};       }
@\halfline@
template<typename E> using ResultType = E::result_type;
\end{lstlisting}

\noindent
The constraint is, again, semantic, and the classes claiming to satisfy it must 
assert it through the \code{is_expression<E>} trait. The template alias \code{ResultType<E>} 
is defined to return the expression's associated type \code{result_type}, which 
defines the type of the result of a lazily-evaluated expression. Any class 
satisfying the \code{LazyExpression} constraint must also provide an implementation 
of the function \code{eval} that evaluates the result of the expression. Conversion 
to the \code{result_type} should call \code{eval} on the object in order to 
allow the use of lazily-evaluated expressions in the contexts where their 
eagerly-evaluated value is expected, e.g. a non-pattern-matching context of the 
right-hand side of the \code{Case} clause. 

Our implementation of the variable pattern \code{var<T>} satisfies the 
\code{Pattern} and \code{LazyExpression} constraints as follows:

\begin{lstlisting}[keepspaces]
template <Regular T> struct var {
  template <typename> 
    struct accepted_type_for { typedef T type; };
  bool operator()(const T& t) const // exact match
    { m_value = t; return true; }
  template <Regular S> 
  bool operator()(const S& s) const // with conversion
    { m_value = s; return m_value == s; }
  typedef T result_type; // type when used in expression
  friend const result_type& eval(const var& v) // eager eval
    { return v.m_value; }
  operator result_type() const { return eval(*this); }
  mutable T m_value; // value bound during matching
};
@\halfline@
template<Regular T>struct    is_pattern<var<T>>:true_type{};
template<Regular T>struct is_expression<var<T>>:true_type{};
\end{lstlisting}

%Each of our six pattern kinds implements the application operator according to 
%the semantics presented in Figure~\ref{exprsem}. The application operator's 
%result has to be convertible to bool; \code{true} indicates a successful match. 
%A class might have several overloads of the above operator that distinguish 
%cases of interest. We summarize the requirements on template parameters of each 
%of our pattern in Figure~\ref{xt-reqs}.
%
%\begin{figure}[h]
%\centering
%\begin{tabular}{llll}
%{\bf Pattern}       & {\bf Parameters}          & {\bf Argument of application operator U}         \\ \hline
%\code{wildcard}     & --                        & --                                               \\
%\code{value<T>}     & \code{Regular<T>}         & \code{Convertible<U,T>}                          \\
%\code{variable<T>}  & \code{Regular<T>}         & \code{Convertible<U,T>}                          \\
%\code{expr<F,E...>} & \code{LazyExpression<E>}  & \code{Convertible<U,expr<F,E...>::result_type>}  \\
%\code{guard<E1,E2>} & \code{LazyExpression<Ei>} & any type accepted by \code{E1::operator()}       \\
%\code{ctor<T,E...>} & \code{Polymorphic<T>}     & \code{Polymorphic<U>} for open encoding          \\
%                    & \code{Object<T>}          & \code{is_base_and_derived<U,T>} for tag encoding \\
%\end{tabular}
%\caption{Requirements on parameters and argument type of an application operator}
%\label{xt-reqs}
%\end{figure}

\noindent
For semantic or efficiency reasons a pattern may have several overloads 
of the application operator. In the example, the first alternative is used when no 
conversion is required; thus, the variable pattern is guaranteed to be accepted.
The second may involve a (possibly-narrowing) conversion, which is why we check 
that the values compare as equal after assignment. Similarly, for type checking 
reasons, \code{accepted_type_for} may (and typically will) provide several partial 
or full specializations to limit the set of acceptable subjects. For example, the
\subterm{pattern combinator}{address combinator} can only be applied to subjects 
of pointer types, so its implementation will report a compile-time error when 
applied to any non-pointer type. 
%Its implementation manifests this by deriving unrestricted case of the type function 
%\code{accepted_type_for} from \code{invalid_subject_type<S>}. This will trigger 
%a static assertion when its associated type \code{type} gets instantiated, 
%resulting in a compile-time error that states that a given subject type \code{S} 
%cannot be used as an argument of the address pattern. The second case of the 
%type function indicates through partial specialization of class templates that 
%for any subject of a pointer type \code{S*}, the accepted type is going to be a 
%pointer to the type accepted by the argument pattern \code{P1} of the address 
%combinator.
%
%\begin{lstlisting}
%template <Pattern P1>
%struct address
%{ // ...
%  template <typename S> 
%    struct accepted_type_for : invalid_subject_type<S> {};
%  template <typename S> struct accepted_type_for<S*> {
%    typedef typename P1::template 
%      accepted_type_for<S>::type* type;
%  };
%  template <typename S>
%    bool operator()(const S* s) const 
%      { return s && m_p1(*s); }
%  P1 m_p1;
%};
%\end{lstlisting}
%
%\noindent
%Checking whether a given subject type can be accepted is inherently late and 
%happens at instantiation time of the nested \code{accepted_type_for} type 
%function and possibly parameterized application operator. For this reason, 
%pattern's implementation may have to provide a set of overloads of the 
%application operator that will be able to accept all possible outcomes of 
%\code{accepted_type_for<S>::type} on any valid subject type \code{S}.

To capture the structure of an expression, the library employs a commonly-used 
technique called ``expression templates''~\cite{Veldhuizen95expressiontemplates, 
vandevoorde2003c++}. %It captures the structure of expression through the type, 
%which for binary addition may look as following:
%
%\begin{lstlisting}[keepspaces,columns=flexible]
%template <LazyExpression E1, LazyExpression E2>
%struct plus {
%  E1 m_e1; E2 m_e2; // subexpressions
%  plus(const E1& e1, const E2& e2) : m_e1(e1), m_e2(e2) {}
%  typedef decltype(std::declval<E1::result_type>() 
%                 + std::declval<E2::result_type>()
%                  ) result_type; // type of result
%  friend result_type eval(const plus& e) 
%    { return eval(e.m_e1) + eval(e.m_e2); }
%  friend plus operator+(const E1& e1, const E2& e2) 
%    { return plus(e1,e2); }
%};
%\end{lstlisting}
%
%\noindent
%The user of the library never sees this definition, instead she implicitly 
%creates its objects with the help of overloaded \code{operator+} on any 
%\code{LazyExpression} arguments. The type itself models the \code{LazyExpression} 
%concept as well so that the lazy expressions can be composed. Notice that all 
%the requirements of the concept are implemented in terms of the requirements 
%on the types of the arguments. The key point to the efficiency of expression 
%templates is that all the types in the final expression are known at compile 
%time, while all the function calls are trivial and fully inlined. Use of new 
%\Cpp{}11 features like move constructors and perfect forwarding allows us to 
%ensure further that no temporary objects will ever be created at run-time and 
%that the evaluation of the expression template will be as efficient as a hand 
%coded function.
%
In general, an \term{expression template} is an algebraic structure $\langle \Sigma_\zeta,\{f_1,f_2,...\}\rangle$ 
defined over the set $\Sigma_\zeta = \{\tau~|~\tau \models \zeta\}$ of all the types $\tau$ 
modeling a given concept $\zeta$. The operations $f_i$ allow one to compose new types  
modeling the concept $\zeta$ out of existing types. In this sense, the types of all lazy 
expressions in \emph{Mach7} stem from a set of a few (possibly-parameterized) basic 
types like \code{var<T>} and \code{value<T>} (which both model \code{LazyExpression}) 
by applying type functors like \code{plus} and \code{minus} to them. Every type 
in the resulting family then has a function \code{eval} defined on it that 
returns a value of the associated type \code{result_type}. Similarly, the types 
of all the patterns stem from a set of a few (possibly-parameterized) patterns like 
\code{wildcard}, \code{var<T>}, \code{value<T>}, \code{C<T>} etc. by applying to 
them pattern combinators such as \code{conjunction}, \code{disjunction}, 
\code{equivalence}, \code{address} etc. The user is allowed to extend both 
algebras with either basic expressions and patterns or with functors and combinators. 

The sets $\Sigma_{LazyExpression}$ and $\Sigma_{Pattern}$ have a non-empty intersection, which 
slightly complicates matters. The basic types \code{var<T>} and \code{value<T>} 
belong to both of those sets, and so do some of the combinators, e.g. 
\code{conjunction}. Since we can only have one overloaded \code{operator&&} for 
a given combination of argument types, we have to state conditionally whether the
requirements of \code{Pattern}, \code{LazyExpression}, or both are satisfied in a 
given instantiation of \code{conjunction<T1,T2>}, depending on what combination 
of these concepts the argument types \code{T1} and \code{T2} model. Concepts, 
unlike interfaces, allow modeling such behavior without multiplying 
implementations or introducing dependencies.

\subsection{Structural Decomposition}
\label{sec:bnd}

\emph{Mach7}'s constructor patterns \code{C<T>(P1,...,Pn)} requires the 
library to know which member of class \code{T} should be used as the subject to 
$P_1$, which should be matched against $P_2$, etc. In functional languages 
supporting algebraic data types, such decomposition is unambiguous as each 
variant has only one constructor, which is thus also used as a \subterm{constructor}{deconstructor}~\cite{padl08,Thorn2012} to define the
decomposition of that type through pattern matching. In \Cpp{}, a class may have 
several constructors, so we must be explicit about a class' decomposition.
We specify that by specializing the library template class \code{bindings}. 
Here are the definitions that are required in order to be able to decompose the 
lambda terms we introduced in \textsection\ref{sec:cpppat}:

\begin{lstlisting}
template<>class bindings<Var>{Members(Var::name);};
template<>class bindings<Abs>{Members(Abs::var,Abs::body);};
template<>class bindings<App>{Members(App::func,App::arg);};
\end{lstlisting}

\noindent
The variadic macro \code{Members} simply expands each of its arguments into the 
following definition, demonstrated here on \code{App::func}:

\begin{lstlisting}
static decltype(&App::func) member1(){return &App::func;}
\end{lstlisting}

\noindent
Each such function returns a pointer-to-member that should be bound in 
position $i$. The library applies them to the subject in order to obtain 
subjects for the sub-patterns $P_1,...,P_n$. 
%Calls to these functions get inlined so that the code to access a member in a 
%given position becomes equivalent to the code to access that member directly. 
Note that binding definitions made this way  
are \emph{non-intrusive} since the original class definition is not touched. 
The binding definitions also respect \emph{encapsulation} since only the public members of the 
target type will be accessible from within a specialization of \code{bindings}. 
Members do not have to be data members only, which can be inaccessible, but any 
of the following three categories:

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item a data member of the target type $T$
\item a nullary member function of the target type $T$
\item a unary external function taking the target type $T$ by pointer, reference, or value.
\end{compactitem}

\noindent
Unfortunately, \Cpp{} does not yet provide sufficient compile-time 
introspection capabilities to let the library generate \code{bindings} 
implicitly. These \code{bindings}, however, only need to be written once for 
a given class hierarchy (e.g. by its designer) and can be reused everywhere. 
This is also true for parameterized classes (\textsection\ref{sec:view}). 

\subsection{Algebraic Decomposition}
\label{sec:slv}

Traditional approaches to generalizing n+k patterns treat matching a pattern 
$f(x,y)$ against a value $r$ as solving an equation $f(x,y)=r$~\cite{OosterhofThesis}. 
This interpretation is well-defined when there are zero or one solutions,
but alternative interpretations are possible when there are multiple solutions. 
Instead of discussing which interpretation is the most general or appropriate, 
we look at n+k patterns as a \term{notational decomposition} of 
mathematical objects. The~elements of the notation are associated with 
sub-components of the matched mathematical entity, which effectively lets us 
decompose it into parts. The structure of the expression tree used in the notation
is an analog of a constructor symbol in structural decomposition, while its 
leaves are placeholders for parameters to be matched against or inferred from 
the mathematical object in question. In~essence, \term{algebraic decomposition} 
is to mathematical objects what structural decomposition is to algebraic data 
types. While the analogy is somewhat ad-hoc, it resembles the situation with 
operator overloading: you do not strictly need it, but it is so %syntactically 
convenient it is virtually impossible not to have it. We demonstrate this 
alternative interpretation of the n+k patterns with examples.

\begin{compactitem}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item An expression $n/m$ is often used to decompose a rational number into 
      numerator and denominator.
\item An expression of the form $3q + r$ can be used to obtain the quotient and 
      remainder of dividing by 3. When $r$ is a constant, it can also be used to
      check membership in a congruence class.
\item The Euler notation $a+bi$, with $i$ being the imaginary unit, is used to 
      decompose a complex number into real and imaginary parts. Similarly, 
      expressions $r(cos \phi + i\mathrm{sin} \phi)$ and $re^{i\phi}$ are used to 
      decompose it into polar form.
\item A 2D line can be decomposed with the slope-intercept form $mX+c$, the 
      linear equation form $aX+bY=c$, or the two-points form $(Y-y_0)(x_1-x_0)=(y_1-y_0)(X-x_0)$.
\item An object representing a polynomial can be decomposed for a specific degree: 
      $a_0$, $a_1X^1+a_0$, $a_2X^2+a_1X^1+a_0$, etc.
\item An element of a vector space can be decomposed along some sub-spaces of 
      interest. For example a 2D vector can be matched against $(0,0)$, $aX$, 
      $bY$, or $aX+bY$ to separate the general case from cases when one or both
      components of the vector are $0$.
\end{compactitem}

\noindent
The expressions $i$, $X$, and $Y$ in those examples are not variables, but rather are named 
constants of some dedicated type that allows the expression to be generically 
decomposed into orthogonal parts. 

The linear equation and two-point forms for decomposing lines already include 
an equality sign, so it is hard to give them semantics in an equational 
approach. In our library that equality sign is not different from any other 
operator, like $+$ or $*$, and is only used to capture the structure of the 
expression, while the exact semantics of matching against that expression is 
given by the user. This~flexibility allows us to generically encode many of the interesting cases 
of the equational approach. The following example, written with use of 
\emph{Mach7}, defines a function for fast computation of Fibonacci numbers by using 
generalized n+k patterns:

\begin{lstlisting}[keepspaces]
int fib(int n) {
  var<int> mm;
  Match(n) {
    Case(any({1,2})) return 1;     
    Case(2*mm)         return sqr(fib(mm+1)) - sqr(fib(mm-1));
    Case(2*mm+1)     return sqr(fib(mm+1)) + sqr(fib(mm));
  } EndMatch               // sqr(x) = x*x
}
\end{lstlisting}

%Applying equational approach to floating-point arithmetic creates even more 
%problems. Even when the solution is unique, it may not be representable by 
%a given floating-point type and thus not satisfy the equation. Once we settle 
%for an approximation, we open ourselves to even more decompositions that become 
%possible with our approach.
%
%\begin{compactitem}
%\setlength{\itemsep}{0pt}
%\setlength{\parskip}{0pt}
%\item Matching $n/m$ with integer variables $n$ and $m$ against a floating-point 
%      value can be given semantics of finding the closest fraction to the 
%      value.
%\item Matching an object representing sampling of some random variable against
%      expressions like $Gaussian(\mu,\sigma^2)$, $Poisson(\lambda)$ or 
%      $Binomial(n,p)$ can be seen as distribution fitting. 
%\item Any curve fitting in this sense becomes an application of pattern 
%      matching. Precision in this case can be a global constant or explicitly 
%      passed parameter of the matching expression.
%\end{compactitem}

%\noindent
%We can make several observations from these examples:

%\begin{compactitem}
%\setlength{\itemsep}{0pt}
%\setlength{\parskip}{0pt}
%\item We might need to have the entire expression available to us in order to 
%      decompose its parts.
%\item Matching the same expression can have different meanings depending on 
%      types of objects composing the expression and the expected result. 
%\item An algorithm to decompose a given expression may depend on the types of 
%      objects in it and the type of the result. 
%\end{compactitem}

%\subsubsection{Solvers}

\noindent
The \emph{Mach7} library already takes care of capturing the structure of lazy expressions 
(i.e. terms). To implement the semantics of their matching, the \emph{Mach7} user (i.e. the designer 
of a concrete notation) writes a new function overload to define the semantics of decomposing a value of a given 
type \code{S} against a term \code{E}:

\begin{lstlisting}[keepspaces]
template <LazyExpression E, typename S> 
  bool solve(const E&, const S&);
\end{lstlisting}

\noindent
The first argument of the function takes an expression template representing a 
term we are matching against, while the second argument represents the expected 
result. Note that even though the first argument is passed in with the \code{const} qualifier, 
it may still modify state in \code{E}. For example, when \code{E} is 
\code{var<T>}, the application operator for const-object that will eventually be 
called will update a mutable member \code{m_value}.
%
The following example defines a generic solver for multiplication by a 
constant $c \neq 0$ of an expression $e=e_1*c$. 

\begin{lstlisting}[keepspaces]
template <LazyExpression E, typename T> 
    requires Field<E::result_type>()
bool solve(const mult<E,value<T>>&e,const E::result_type&r)
    { return solve(e.m_e1,r/eval(e.m_e2)); } // e.m_e2 is @$c$@
@\halfline@
template <LazyExpression E, typename T>
    requires Integral<E::result_type>()
bool solve(const mult<E,value<T>>&e,const E::result_type&r){
    T c = eval(e.m_e2); // e.m_e2 is @$c$@
    return r%c == 0 && solve(e.m_e1,r/c);
}
\end{lstlisting}

\noindent
Intuitively, matching $e_1*c$ against the value $r$ in the equational approach means 
solving $e_1*c=r$, which means that we should try matching the sub-expression 
$e_1$ against $\frac{r}{c}$.

The first overload is only applicable when the result type of the 
sub-expression models the \code{Field} concept. In this case, we can rely on the 
presence of a unique inverse and simply call division without any additional 
checks. The second overload uses integer division, which does not guarantee the 
unique inverse, and thus we have to verify that the result is divisible by the 
constant first. This~last overload combined with a similar solver for addition 
of integral types is everything the library needs to support the \code{fib} example.
%definition of the \code{fib} function from \textsection\ref{sec:cpppat}. This
%demonstrates how an equational approach can be generically implemented for a 
%number of expressions.

%A generic solver capable of decomposing a complex value using the Euler 
%notation is very easy to define by fixing the structure of expression:
%
%\begin{lstlisting}[keepspaces]
%template <LazyExpression E1, LazyExpression E2> 
%    requires SameType<E1::result_type,E2::result_type>()
%bool solve(
%    const plus<mult<E1,value<complex<E1::result_type>>>,E2>& e, 
%    const complex<E1::result_type>& r);
%\end{lstlisting}
%
%\noindent
%As we mentioned in \textsection\ref{sec:cpppat}, the template facilities of 
%\Cpp{} resemble pattern-matching facilities of other languages. Here, we 
%essentially use these compile-time patterns to describe the structure of the 
%expression this solver is applicable to: $e_1*c+e_2$ with types of $e_1$ and 
%$e_2$ being the same as type on which a complex value $c$ is defined. The actual 
%value of the complex constant $c$ will not be known until run-time, but assuming 
%its imaginary part is not $0$, we will be able to generically obtain the values 
%for sub-expressions.

%% Our approach is largely possible due to the fact that the library only serves as 
%% an interface between expressions and functions defining their semantics and 
%% algebraic decomposition. The fact that the user explicitly defines the variables 
%% she would like to use in patterns is also a key as it lets us specialize not 
%% only on the structure of the expression, but also on the types involved. 
%% Inference of such types in functional languages would be hard or impossible as the 
%% expression may have entirely different semantics depending on the types of 
%% arguments involved. Concept-based overloading simplifies significantly the case 
%% analysis on the properties of types, making the solvers generic and composable.
%% The approach is also viable as expressions are decomposed at compile-time and 
%% not at run-time, letting the compiler inline the entire composition of solvers. 

%An obvious disadvantage of this approach is that the more complex expression 
%becomes, the more overloads the user will have to provide to cover all 
%expressions of interest. The set of overloads will also have to be made 
%unambiguous for any given expression, which may be challenging for novices. An 
%important restriction of this approach is its inability to detect multiple uses 
%of the same variable in an expression at compile time. This happens because 
%expression templates remember the form of an expression in a type, so use of two 
%variables of the same type is indistinguishable from the use of the same 
%variable twice. This can be worked around by giving different variables 
%(slightly) different types or making additional checks as to the structure of 
%expression at run-time, but that will make the library even more verbose or 
%incur a significant run-time overhead.

\subsection{Views}
\label{sec:view}

Any type $T$ may have an arbitrary number of \term{binding}s associated with it, 
which are specified by varying the second parameter of the \code{bindings} 
template: \term{layout}. The layout is a non-type template parameter of 
integral type; the layout parameter has a default value and is thus omitted most of the time.
Our library's support of multiple bindings (through layouts) effectively enables 
a facility similar to Wadler's \subterm{pattern}{views}\cite{Wadler87}. Consider:

\begin{lstlisting}[keepspaces]
enum { cartesian = default_layout, polar }; // Layouts
@\halfline@
template <class T> struct bindings<std::complex<T>>
  { Members(std::real<T>,std::imag<T>); };
template <class T> struct bindings<std::complex<T>, polar>
  { Members(std::abs<T>,std::arg<T>); };
@\halfline@
template <class T> using Cart = view<std::complex<T>>;
template <class T> using Pole = view<std::complex<T>,polar>;
@\halfline@
  std::complex<double> c; double a,b,r,f;
  Match(c)
    Case(Cart<double>>(a,b)) ... // default layout
    Case(Pole<double>>(r,f))  ... // view for polar layout
  EndMatch
\end{lstlisting}

\noindent
The \Cpp{} standard effectively forces the standard library to use the Cartesian 
representation~\cite[\textsection26.4-4]{C++11}, which is why we chose the 
\code{Cart} layout as the default. We then define bindings for each 
layout and introduce template aliases (an analog of typedefs for parameterized 
classes) for each view. The \emph{Mach7} class \code{view<T,l>} binds a 
target type with one of that type's layouts.  \code{view<T,l>} can be used everywhere the
original target type \code{T} was expected.

The important difference from Wadler's solution is that our views can only be 
used in a pattern-matching context, not as constructors or as arguments to functions.

\subsection{Match Statement}
\label{sec:matchstmt}

In functional languages with built-in pattern matching, \emph{relational 
matching} on multiple subjects is usually reduced to \emph{nested matching} on a 
single subject by wrapping multiple arguments into a tuple. In a library setting, 
we are able to provide a more efficient implementation if we keep the arguments 
separated. This is why our \code{Match} statement extends the efficient type 
switch for \Cpp{}~\cite{TS12} to handle multiple subjects (both polymorphic and 
non-polymorphic) (\textsection\ref{sec:multiarg}) and to accept patterns in case 
clauses (\textsection\ref{sec:patcases}).
%The first extension enables efficient \emph{relational matching}, while the second enables \emph{nesting of patterns}. 

\subsubsection{Multi-argument Type Switching}
\label{sec:multiarg}

The core of our efficient type switch~\cite{TS12} is based on the fact that 
virtual table pointers (vtbl-pointers) uniquely identify subobjects 
in the object and are perfect for hashing. Open type switch maps these 
vtbl-pointers to jump targets and necessary this-pointer offsets and provides an 
amortized constant-time dispatch to the appropriate case clause. Its 
efficiency relies on the optimal hash function $H_{kl}^V$ built for a set of 
vtbl-pointers $V$ seen by a type switch. It is chosen by varying the parameters $k$ 
and $l$ to minimize the probability of conflict. The parameter $k$ represents the 
logarithm of the size of cache, while the parameter $l$ is the number of 
low bits to ignore.

%We considered two different approaches to extending that solution to $N$ 
%arguments. The first approach was based on maintaining an $N$-dimensional 
%table indexed by independent $H_{k_il_i}^{V_i}$ maintained for each of the 
%arguments $i$. The second approach was to aggregate the information from 
%multiple vtbl-pointers into a single hash in a hope the hashing would still 
%maintain its favorable properties. The first approach requires amount of memory 
%proportional to $O(|V|^N)$ regardless of how many different combinations of 
%vtbl-pointers came through the statement. The second approach requires the 
%amount of memory linear in the number of vtbl-pointer combinations seen, which 
%in the worst case becomes the same $O(|V|^N)$. The first approach requires 
%lookup in $N$ caches, with each lookup being a subject to potential collisions; 
%the second approach requires non-trivial computations to aggregate $N$ 
%vtbl-pointers into a single hash value and may result in potentially more 
%collisions in comparison to the first approach. Our experience of dealing with 
%multiple dispatch in \Cpp{} suggests that we rarely see all combinations of 
%types coming through a given multi-method in real-world applications. With this 
%in mind, we did not expect all combination of types come through a given 
%\code{Match} statement and thus preferred the second solution, which grows 
%linearly in memory with the number of combinations seen.

A \emph{Morton order} (aka \emph{Z-order}) is a function that 
maps multidimensional data to one dimension while preserving the locality of the 
data points~\cite{Morton66}. A Morton number of an $N$-dimensional coordinate 
point is obtained by interleaving the binary representations of all coordinates.
The original one-dimensional hash function $H_{kl}^V$ applied to arguments $v \in V$ 
produced hash values in a tight range $[0..2^k[$ where $k \in [K,K+1]$ for 
$2^{K-1} < |V| \leq 2^K$. The produced values were close to each other, which 
improved the cache hit rate due to increased locality of reference. The 
idea is thus to use Morton order on these hash values -- not on the original 
vtbl-pointers -- in order to preserve locality of reference. To do this, we 
retain a single parameter $k$ reflecting the size of the cache, but we 
keep $N$ optimal offsets $l_i$ for each argument $i$.

Consider a set $V^N = \{\tpl{v_1^1,...,v_1^N},...,\tpl{v_n^1,...,v_n^N}\}$ of 
$N$-dimensional tuples representing the set of vtbl-pointer combinations coming 
through a given \code{Match} statement. As with the one-dimensional case, we 
restrict the size $2^k$ of the cache to be not larger than twice the closest 
power of two greater or equal to $n=|V^N|$: i.e. $k \in [K,K+1]$, where 
$2^{K-1} < |V^N| \leq 2^K$. For a given $k$ and offsets $l_1,...,l_N$ a hash 
value of a given combination $\tpl{v^1,...,v^N}$ is defined as 
$H_{kl_1...l_N}(\tpl{v^1,...,v^N})=\mu(\frac{v^1}{2^{l_1}},...,\frac{v^N}{2^{l_N}}) \mod 2^k$, 
where the function $\mu$ returns the Morton number (bit interleaving) of $N$ numbers.
 
As in the one-dimensional case, we vary the parameters $k$,$l_1$,$...$,$l_N$ in 
their finite and small domains to obtain an optimal hash function 
$H^{V^N}_{kl_1...l_N}$ by minimizing the probability of conflict on values from 
$V^N$. Unlike the one-dimensional case, we do not try to find the optimal 
parameters every time we reconfigure the cache. Instead, we only try to improve 
the parameters to render fewer conflicts in comparison to the number of conflicts 
rendered by the current configuration. This does not prevent us from eventually 
converging to the same optimal parameters, which we do over time, but is 
important for holding constant the amortized complexity of the access. 
%Observe that the domain of each parameter of the optimal hash function 
%$H^{V^N}_{kl_1...l_N}$ only grows since $V^N$ only grows, while any cache 
%configuration is also a valid cache configuration in a larger cache, rendering 
%the same number of conflicts.
We demonstrate in \textsection\ref{sec:morton} that -- similarly to the one-dimensional 
case -- such a hash function produces few collisions on real-world class 
hierarchies, and yet it is simple enough to compute that it competes well with alternatives 
that can cope with relational matching.

%In practice, the library does not consider all $N$ arguments of a given 
%\code{Match} statement, but only the $M$ polymorphic arguments ($M \leq N$). It 
%then builds an efficient type switch based on those $M$ arguments. The type 
%switch guarantees efficient dispatch to the first case clause that can possibly 
%handle a given combination of arguments based on the subset of only polymorphic 
%arguments. The patterns are then tried sequentially. The underlying type switch 
%uses pattern's type-function \code{accepted_type_for<Si>} instantiated with the 
%subject type $Si$ of a given argument $i$ in order to obtain the target type 
%requested by the pattern in that position.

\subsubsection{Support for Patterns}
\label{sec:patcases}

Given a statement \code{Match(e_1,...,e_N)} applied to arbitrary expressions $e_i$, the library introduces several 
names into the scope of the statement: e.g. the number of arguments $N$, the subject 
types \code{subject_type_ii} (defined as \code{decltype(e_ii)} modulo type 
qualifiers), and the number of polymorphic arguments $M$. When $M > 0$ it also 
introduces the necessary data structures to implement efficient type 
switching~\cite{TS12}. Only the $M$ arguments whose \code{subject_type_ii} are 
polymorphic will be used for fast type switching.

For each case clause \code{Case(p_1,...,p_N)} the library ensures that the 
number of arguments to the case clause $N$ matches the number of arguments to 
the \code{Match} statement, and that the type \code{P_ii} of every expression 
\code{p_ii} passed as its argument models the \code{Pattern} concept. 
%Initially we allowed case clauses to accept less than $N$ patterns, assuming the 
%missing patterns to be the wildcard, however, brittleness of the macro system 
%made us reconsider this. The problem is that macro system is blind to \Cpp{} 
%syntax and template instantiation like \code{A<B,C>} used in a pattern will be 
%treated by the preprocessor as 2 macro arguments. This resulted in errors that 
%were hard for the users to comprehend.
For each \code{subject_type_ii} it introduces \code{target_type_ii} -- 
the result of evaluating the type function \code{AcceptedType<P_ii,subject_type_ii>} --
into the scope of the case clause.
This is the type the pattern 
expects as an argument on a subject of type \code{subject_type_ii} (\textsection\ref{sec:pat}), 
which is used by the type switching mechanism to properly cast the subject if necessary. 
The library then introduces the names \code{match_ii} of type \code{target_type_ii&} 
bound to properly casted subjects and available to the user in the right-hand 
side of the case clause in the event of a successful match. The qualifiers applied to 
the type of \code{match_ii} reflect the qualifiers applied to the type of the subject 
\code{e_ii}. Finally, the library generates code that sequentially applies 
each pattern to properly-casted subjects, making the clause's body conditional:

\begin{lstlisting}
if (p_1(match_1) && ... && p_N(match_N)) { /* body */ }
\end{lstlisting}

\noindent
When type switching is not involved, the generated code implements the na\"ive 
backtracking strategy, which is known to be inefficient as it can produce 
redundant computations~\cite[\textsection 5]{Cardelli84}. More-efficient 
algorithms for compiling pattern matching have been developed 
since~\cite{Augustsson85,Maranget92,Puel93,OPM01,Maranget08}. Unfortunately, while these 
algorithms cover most of the typical kinds of patterns, they are not pattern-agnostic 
as they make assumptions about the semantics of concrete patterns. A library-based 
approach to pattern matching is agnostic of the semantics of any given 
user-defined pattern. The interesting research question in this context would 
be: what language support is required to be able to optimize open patterns? 
%While we do not address this question in its generality, our solution makes a 
%small step in that direction.

The main advantage from using pattern matching in \emph{Mach7} comes from the fast type 
switching weaved into the \code{Match} statement. It effectively skips case 
clauses that will definitely be rejected because their target type is not one 
of the subject's dynamic types. Of course, this is only applicable to polymorphic 
arguments; for non-polymorphic arguments, the matching is done na\"ively with a
cascade of conditional statements.
